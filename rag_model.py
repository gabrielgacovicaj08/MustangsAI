import os
import langchain
from langchain import hub
from langchain_core.output_parsers import StrOutputParser
from langchain.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_openai import ChatOpenAI, OpenAIEmbeddings


class RAGModel:
    def __init__(self, vector_store, openai_api_key, collection_name="msu_collection", k=3):
        """
        Initialize the RAGModel with the required parameters.

        Parameters:
        - vector_store: The Chroma vector store instance.
        - openai_api_key: OpenAI API key for LLM access.
        - collection_name: Name of the collection in the vector store (default is "default_collection").
        - k: Number of documents to retrieve (default is 6).
        """
        self.vector_store = vector_store
        self.openai_api_key = openai_api_key
        self.collection_name = collection_name
        self.k = k

        # Initialize the LLM
        self.llm = ChatOpenAI(api_key=self.openai_api_key, model="gpt-4o-mini")

        # Initialize the retriever from the vector store
        self.retriever = self.vector_store.as_retriever(
            search_type="similarity",
            search_kwargs={"k": self.k}
        )

        # Pull the prompt or use a fallback
        self.prompt = self._get_prompt()

    def _get_prompt(self):
        try:
            self.template = """Use the following pieces of context to answer the question at the end.
            If you don't know the answer, just say that you don't know, don't try to make up an answer.
            Use as many information as needed but try to be as precise and accurate as you can.
            If you need make a enumerate list.

            {context}

            Question: {question}

            Helpful Answer:"""
            custom_rag_prompt = PromptTemplate.from_template(self.template)
            
            return custom_rag_prompt
        except Exception as e:
            print(f"Error setting up the prompt {e}. ")

       

    @staticmethod
    def format_docs(docs):
        """Format the retrieved documents into a single string."""
        return "\n\n".join(doc.page_content for doc in docs)

    def respond(self, query):
        """
        Generate a response based on the query using the RAG pipeline.

        Parameters:
        - query: The user query string.

        Returns:
        - response: The response generated by the LLM.
        """
            
        try:
            
            # Prepare the input for the RAG chain
            rag_chain = (
                {"context": self.retriever | self.format_docs, "question": RunnablePassthrough()}
                | self.prompt
                | self.llm
                | StrOutputParser()
            )

            # Invoke the chain and return the response
            response = rag_chain.invoke(query)
            return response
        except Exception as e:
            print(f"Error generating response: {e}")
            return "Sorry, I couldn't process your request at the moment."



